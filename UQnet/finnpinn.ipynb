{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.optimize import bisect\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "from lib import AnalyticRetardation, load_data, Initialize, Flux_Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caps_calculation(network_preds: dict[str, Any], c_up, c_down, Y, verbose=0):\n",
    "    \"\"\"Caps calculations for single quantile\"\"\"\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\"--- Start caps calculations for SINGLE quantile ---\")\n",
    "        print(\"**************** For Training data *****************\")\n",
    "\n",
    "    if len(Y.shape) == 2:\n",
    "        Y = Y.flatten()\n",
    "\n",
    "    bound_up = (network_preds[\"mean\"] + c_up * network_preds[\"up\"]).numpy().flatten()\n",
    "    bound_down = (\n",
    "        (network_preds[\"mean\"] - c_down * network_preds[\"down\"]).numpy().flatten()\n",
    "    )\n",
    "\n",
    "    y_U_cap = bound_up > Y  # y_U_cap\n",
    "    y_L_cap = bound_down < Y  # y_L_cap\n",
    "\n",
    "    y_all_cap = np.logical_or(y_U_cap, y_L_cap)  # y_all_cap\n",
    "    PICP = np.count_nonzero(y_all_cap) / y_L_cap.shape[0]  # 0-1\n",
    "    MPIW = np.mean(\n",
    "        (network_preds[\"mean\"] + c_up * network_preds[\"up\"]).numpy().flatten()\n",
    "        - (network_preds[\"mean\"] - c_down * network_preds[\"down\"]).numpy().flatten()\n",
    "    )\n",
    "    if verbose > 0:\n",
    "        print(f\"Num of train in y_U_cap: {np.count_nonzero(y_U_cap)}\")\n",
    "        print(f\"Num of train in y_L_cap: {np.count_nonzero(y_L_cap)}\")\n",
    "        print(f\"Num of train in y_all_cap: {np.count_nonzero(y_all_cap)}\")\n",
    "        print(f\"np.sum results(train): {np.sum(y_all_cap)}\")\n",
    "        print(f\"PICP: {PICP}\")\n",
    "        print(f\"MPIW: {MPIW}\")\n",
    "\n",
    "    return (\n",
    "        PICP,\n",
    "        MPIW,\n",
    "    )\n",
    "\n",
    "\n",
    "def optimize_bound(\n",
    "    *,\n",
    "    mode: str,\n",
    "    y_train: np.ndarray,\n",
    "    pred_mean: np.ndarray,\n",
    "    pred_std: np.ndarray,\n",
    "    num_outliers: int,\n",
    "    c0: float = 0.0,\n",
    "    c1: float = 1e5,\n",
    "    maxiter: int = 1000,\n",
    "    verbose=0,\n",
    "):\n",
    "    def count_exceeding_upper_bound(c: float):\n",
    "        bound = pred_mean + c * pred_std\n",
    "        f = np.count_nonzero(y_train >= bound) - num_outliers\n",
    "        return f\n",
    "\n",
    "    def count_exceeding_lower_bound(c: float):\n",
    "        bound = pred_mean - c * pred_std\n",
    "        f = np.count_nonzero(y_train <= bound) - num_outliers\n",
    "        return f\n",
    "\n",
    "    objective_function = (\n",
    "        count_exceeding_upper_bound if mode == \"up\" else count_exceeding_lower_bound\n",
    "    )\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f\"Initial bounds: [{c0}, {c1}]\")\n",
    "\n",
    "    try:\n",
    "        optimal_c = bisect(objective_function, c0, c1, maxiter=maxiter)\n",
    "        if verbose > 0:\n",
    "            final_count = objective_function(optimal_c)\n",
    "            print(f\"Optimal c: {optimal_c}, Final count: {final_count}\")\n",
    "        return optimal_c\n",
    "    except ValueError as e:\n",
    "        if verbose > 0:\n",
    "            print(f\"Bisect method failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def compute_boundary_factors(\n",
    "    *, y_train: np.ndarray, network_preds: dict[str, Any], quantile: float, verbose=0\n",
    "):\n",
    "    n_train = y_train.shape[0]\n",
    "    num_outlier = int(n_train * (1 - quantile) / 2)\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\n",
    "            \"--- Start boundary optimizations for SINGLE quantile: {}\".format(quantile)\n",
    "        )\n",
    "        print(\n",
    "            \"--- Number of outlier based on the defined quantile: {}\".format(\n",
    "                num_outlier\n",
    "            )\n",
    "        )\n",
    "\n",
    "    c_up, c_down = [\n",
    "        optimize_bound(\n",
    "            y_train=y_train,\n",
    "            pred_mean=network_preds[\"mean\"],\n",
    "            pred_std=network_preds[mode],\n",
    "            mode=mode,\n",
    "            num_outliers=num_outlier,\n",
    "        )\n",
    "        for mode in [\"up\", \"down\"]\n",
    "    ]\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\"--- c_up: {}\".format(c_up))\n",
    "        print(\"--- c_down: {}\".format(c_down))\n",
    "\n",
    "    return c_up, c_down\n",
    "\n",
    "\n",
    "def create_PI_training_data(\n",
    "    network_mean, X, Y\n",
    ") -> tuple[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"Generate up and down training data\"\"\"\n",
    "    threshold = 40  # TODO\n",
    "    with torch.no_grad():\n",
    "        Y_pred = network_mean(X)\n",
    "        diff_train = torch.sum((Y - Y_pred) ** 2, dim=[1, 2, 3])\n",
    "        up_idx = diff_train > threshold\n",
    "        down_idx = diff_train < threshold\n",
    "\n",
    "        X_up = X[up_idx]\n",
    "        Y_up = diff_train[up_idx]\n",
    "\n",
    "        X_down = X[down_idx]\n",
    "        Y_down = -1.0 * diff_train[down_idx]\n",
    "        print(X_down.shape, X_up.shape)\n",
    "        print(Y_down.shape, Y_up.shape)\n",
    "\n",
    "    return ((X_up, Y_up), (X_down, Y_down))\n",
    "\n",
    "\n",
    "def eval_networks(\n",
    "    networks: dict[str, Any], x, as_numpy: bool = False\n",
    ") -> dict[str, Any]:\n",
    "    with torch.no_grad():\n",
    "        d = {k: network(x) for k, network in networks.items()}\n",
    "    if as_numpy:\n",
    "        d = {k: v.numpy() for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "\n",
    "class UQ_Net_mean(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_neurons: list[int], num_inputs, num_outputs, activation=\"relu\"\n",
    "    ):\n",
    "        super(UQ_Net_mean, self).__init__()\n",
    "        self.activation_fun = torch.relu if activation == \"relu\" else torch.tanh\n",
    "\n",
    "        self.inputLayer = nn.Linear(num_inputs, num_neurons[0])\n",
    "        self.fcs = nn.ModuleList()\n",
    "        for i in range(len(num_neurons) - 1):\n",
    "            self.fcs.append(nn.Linear(num_neurons[i], num_neurons[i + 1]))\n",
    "        self.outputLayer = nn.Linear(num_neurons[-1], num_outputs)\n",
    "\n",
    "        # Initialize weights with a mean of 0.1 and stddev of 0.1\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0.1, std=0.1)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation_fun(self.inputLayer(x))\n",
    "        for i in range(len(self.fcs)):\n",
    "            x = self.activation_fun(self.fcs[i](x))\n",
    "        x = self.outputLayer(x)\n",
    "        # TODO: Maybe use sigmoid here since we learn inv ret\n",
    "        return x\n",
    "\n",
    "\n",
    "class UQ_Net_std(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_neurons: list[int],\n",
    "        num_inputs,\n",
    "        num_outputs,\n",
    "        net=None,\n",
    "        bias=None,\n",
    "        activation=\"relu\",\n",
    "    ):\n",
    "        super(UQ_Net_std, self).__init__()\n",
    "\n",
    "        self.activation_fun = torch.relu if activation == \"relu\" else torch.tanh\n",
    "        self.inputLayer = nn.Linear(num_inputs, num_neurons[0])\n",
    "        self.fcs = nn.ModuleList()\n",
    "        for i in range(len(num_neurons) - 1):\n",
    "            self.fcs.append(nn.Linear(num_neurons[i], num_neurons[i + 1]))\n",
    "        self.outputLayer = nn.Linear(num_neurons[-1], num_outputs)\n",
    "\n",
    "        # Initialize weights with a mean of 0.1 and stddev of 0.1\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0.1, std=0.1)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "        # Custom bias\n",
    "        if bias is None:\n",
    "            self.custom_bias = torch.nn.Parameter(torch.tensor([3.0]))\n",
    "        else:\n",
    "            self.custom_bias = torch.nn.Parameter(torch.tensor([bias]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation_fun(self.inputLayer(x))\n",
    "        for i in range(len(self.fcs)):\n",
    "            x = self.activation_fun(self.fcs[i](x))\n",
    "        x = self.outputLayer(x)\n",
    "        x = x + self.custom_bias\n",
    "        x = torch.sqrt(torch.square(x) + 0.2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcentrationPredictor(nn.Module):\n",
    "    def __init__(self, u0: torch.Tensor, cfg: Initialize, ret_inv_funs=None):\n",
    "        \"\"\"TODO: Docstring\n",
    "\n",
    "        Args:\n",
    "            u0 (tensor): initial condition, dim: [num_features, Nx]\n",
    "            cfg (_type_): _description_\n",
    "        \"\"\"\n",
    "        super(ConcentrationPredictor, self).__init__()\n",
    "        if ret_inv_funs is None:\n",
    "            ret_inv_funs = [None] * len(u0)\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.u0 = u0\n",
    "        self.dudt_fun = ConcentrationChangeRatePredictor(\n",
    "            u0, cfg, ret_inv_funs=ret_inv_funs\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"Predict the concentration profile at given time steps from an initial condition using the FINN method.\n",
    "\n",
    "        Args:\n",
    "            t (tensor): time steps\n",
    "\n",
    "        Returns:\n",
    "            tensor: Full field solution of concentration at given time steps.\n",
    "        \"\"\"\n",
    "\n",
    "        ode_pred = odeint(self.dudt_fun, self.u0, t, rtol=1e-5, atol=1e-6)\n",
    "        return ode_pred\n",
    "\n",
    "    def run_training(self, t: torch.Tensor, u_full_train: torch.Tensor):\n",
    "        \"\"\"Train to predict the concentration from the given full field training data.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            t (tensor): time steps for integration, dim: [Nt,]\n",
    "            x_train (tensor): full field solution at each time step, dim: [Nt, num_features, Nx]\n",
    "        \"\"\"\n",
    "        out_dir = Path(\"data_out\")\n",
    "        out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        optimizer = torch.optim.LBFGS(self.parameters(), lr=0.1)\n",
    "\n",
    "        u_ret = torch.linspace(0.0, 1.0, 100).view(-1, 1).to(self.cfg.device)\n",
    "        # TODO: Should not be here\n",
    "        ret_linear = AnalyticRetardation.linear(\n",
    "            u_ret, por=self.cfg.por, rho_s=self.cfg.rho_s, Kd=self.cfg.Kd\n",
    "        )\n",
    "        ret_freundlich = AnalyticRetardation.freundlich(\n",
    "            u_ret,\n",
    "            por=self.cfg.por,\n",
    "            rho_s=self.cfg.rho_s,\n",
    "            Kf=self.cfg.Kf,\n",
    "            nf=self.cfg.nf,\n",
    "        )\n",
    "        ret_langmuir = AnalyticRetardation.langmuir(\n",
    "            u_ret,\n",
    "            por=self.cfg.por,\n",
    "            rho_s=self.cfg.rho_s,\n",
    "            smax=self.cfg.smax,\n",
    "            Kl=self.cfg.Kl,\n",
    "        )\n",
    "        np.save(out_dir / \"u_ret.npy\", u_ret)\n",
    "        np.save(out_dir / \"retardation_linear.npy\", ret_linear)\n",
    "        np.save(out_dir / \"retardation_freundlich.npy\", ret_freundlich)\n",
    "        np.save(out_dir / \"retardation_langmuir.npy\", ret_langmuir)\n",
    "\n",
    "        # Define the closure function that consists of resetting the\n",
    "        # gradient buffer, loss function calculation, and backpropagation\n",
    "        # The closure function is necessary for LBFGS optimizer, because\n",
    "        # it requires multiple function evaluations\n",
    "        # The closure function returns the loss value\n",
    "        def closure():\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            ode_pred = self.forward(t)  # aka. y_pred\n",
    "            # TODO: mean instead of sum?\n",
    "            loss = self.cfg.error_mult * torch.sum((u_full_train - ode_pred) ** 2)\n",
    "\n",
    "            # Physical regularization: value of the retardation factor should decrease with increasing concentration\n",
    "            ret_inv_pred = self.retardation_inv_scaled(u_ret)\n",
    "            loss += self.cfg.phys_mult * torch.sum(\n",
    "                torch.relu(ret_inv_pred[:-1] - ret_inv_pred[1:])\n",
    "            )  # TODO: mean instead of sum?\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            return loss\n",
    "\n",
    "        # Iterate until maximum epoch number is reached\n",
    "        for epoch in range(1, self.cfg.epochs + 1):\n",
    "            dt = time.time()\n",
    "            optimizer.step(closure)\n",
    "            loss = closure()\n",
    "            dt = time.time() - dt\n",
    "\n",
    "            print(\n",
    "                f\"Training: Epoch [{epoch + 1}/{self.cfg.epochs}], \"\n",
    "                f\"Training Loss: {loss.item():.4f}, Runtime: {dt:.4f} secs\"\n",
    "            )\n",
    "\n",
    "            ret_pred_path = self.cfg.model_path / f\"retPred_{epoch}.npy\"\n",
    "            np.save(ret_pred_path, self.retardation(u_ret).detach().numpy())\n",
    "\n",
    "    def retardation_inv_scaled(self, u):\n",
    "        return self.dudt_fun.flux_modules[0].ret_inv_fun(u)\n",
    "\n",
    "    def retardation(self, u):\n",
    "        return (\n",
    "            1.0\n",
    "            / self.dudt_fun.flux_modules[0].ret_inv_fun(u)\n",
    "            / 10 ** self.dudt_fun.flux_modules[0].p_exp\n",
    "        )\n",
    "\n",
    "\n",
    "class ConcentrationChangeRatePredictor(nn.Module):\n",
    "    def __init__(self, u0, cfg, ret_inv_funs=None):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        Inputs:\n",
    "            u0      : initial condition, dim: [num_features, Nx]\n",
    "            cfg     : configuration object of the model setup, containing boundary condition types, values, learnable parameter settings, etc.\n",
    "        \"\"\"\n",
    "        if ret_inv_funs is None:\n",
    "            ret_inv_funs = [None] * len(u0)\n",
    "\n",
    "        super(ConcentrationChangeRatePredictor, self).__init__()\n",
    "\n",
    "        self.flux_modules = nn.ModuleList()\n",
    "        self.num_vars = u0.size(0)\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Create flux kernel for each variable to be calculated\n",
    "        for var_idx in range(self.num_vars):\n",
    "            self.flux_modules.append(\n",
    "                Flux_Kernels(\n",
    "                    u0[var_idx], self.cfg, var_idx, ret_inv_fun=ret_inv_funs[var_idx]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, t, u):\n",
    "        \"\"\"Computes du/dt to be put into the ODE solver\n",
    "\n",
    "        Args:\n",
    "            t (float): time point\n",
    "            u (tensor): the unknown variables to be calculated taken from the previous time step, dim: [num_features, Nx]\n",
    "\n",
    "        Returns:\n",
    "            tensor: the time derivative of u (du/dt), dim: [num_features, Nx]\n",
    "        \"\"\"\n",
    "        flux = []\n",
    "\n",
    "        # Use flux and state kernels to calculate du/dt for all unknown variables\n",
    "        for var_idx in range(self.num_vars):\n",
    "            flux.append(\n",
    "                self.flux_modules[var_idx](\n",
    "                    u[self.cfg.flux_calc_idx[var_idx]],\n",
    "                    u[self.cfg.flux_couple_idx[var_idx]],\n",
    "                    t,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        du = torch.stack(flux)\n",
    "\n",
    "        return du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {}\n",
    "configs[\"quantile\"] = 0.95\n",
    "configs[\"Max_iter\"] = 5000\n",
    "SEED = 10\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "cfg = Initialize()\n",
    "u0 = torch.zeros(cfg.num_vars, cfg.Nx, 1)\n",
    "net_mean = ConcentrationPredictor(\n",
    "    u0=u0.clone(),\n",
    "    cfg=cfg,\n",
    "    ret_inv_funs=[\n",
    "        (\n",
    "            UQ_Net_mean(\n",
    "                num_neurons=[15, 15, 15],\n",
    "                num_inputs=1,\n",
    "                num_outputs=1,\n",
    "                activation=\"tanh\",\n",
    "            ).to(cfg.device)\n",
    "            if is_fun\n",
    "            else None\n",
    "        )\n",
    "        for is_fun in cfg.is_retardation_a_func\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "concentration_data = load_data(cfg)\n",
    "t = torch.linspace(0.0, cfg.T, cfg.Nt)\n",
    "# x = torch.linspace(0.0, cfg.X, cfg.Nx)\n",
    "\n",
    "train_split_index = 51\n",
    "x_train = t[:train_split_index]\n",
    "y_train = concentration_data[:train_split_index]\n",
    "x_valid = t[train_split_index:]\n",
    "y_valid = concentration_data[train_split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = create_PI_training_data(net_mean, X=x_train, Y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Mean Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch [2/10], Training Loss: 1953.3463, Runtime: 0.8493 secs\n",
      "Training: Epoch [3/10], Training Loss: 1953.3463, Runtime: 0.7534 secs\n",
      "Training: Epoch [4/10], Training Loss: 1953.3463, Runtime: 0.7622 secs\n",
      "Training: Epoch [5/10], Training Loss: 1953.3463, Runtime: 0.8253 secs\n",
      "Training: Epoch [6/10], Training Loss: 1953.3463, Runtime: 0.7721 secs\n",
      "Training: Epoch [7/10], Training Loss: 1953.3463, Runtime: 0.7673 secs\n",
      "Training: Epoch [8/10], Training Loss: 1953.3463, Runtime: 0.8218 secs\n",
      "Training: Epoch [9/10], Training Loss: 1953.3463, Runtime: 0.7712 secs\n",
      "Training: Epoch [10/10], Training Loss: 1953.3463, Runtime: 0.7689 secs\n",
      "Training: Epoch [11/10], Training Loss: 1953.3463, Runtime: 0.8819 secs\n"
     ]
    }
   ],
   "source": [
    "net_mean.run_training(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Up/Down Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24]) torch.Size([27])\n",
      "torch.Size([24]) torch.Size([27])\n"
     ]
    }
   ],
   "source": [
    "data_train_up, data_train_down = create_PI_training_data(net_mean, X=x_train, Y=y_train)\n",
    "# TODO: Not used yet\n",
    "# data_val_up, data_val_down = create_PI_training_data(\n",
    "#     net_mean, X=x_valid, Y=y_valid\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Up/Down Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_up = ConcentrationPredictor(\n",
    "    u0=u0.clone(),\n",
    "    cfg=cfg,\n",
    "    ret_inv_funs=[\n",
    "        (\n",
    "            UQ_Net_std(\n",
    "                num_neurons=[15, 15, 15],\n",
    "                num_inputs=1,\n",
    "                num_outputs=1,\n",
    "                net=\"up\",\n",
    "                activation=\"tanh\",\n",
    "            ).to(cfg.device)\n",
    "            if is_fun\n",
    "            else None\n",
    "        )\n",
    "        for is_fun in cfg.is_retardation_a_func\n",
    "    ],\n",
    ")\n",
    "net_down = ConcentrationPredictor(\n",
    "    u0=u0.clone(),\n",
    "    cfg=cfg,\n",
    "    ret_inv_funs=[\n",
    "        (\n",
    "            UQ_Net_std(\n",
    "                num_neurons=[15, 15, 15],\n",
    "                num_inputs=1,\n",
    "                num_outputs=1,\n",
    "                net=\"down\",\n",
    "                activation=\"tanh\",\n",
    "            ).to(cfg.device)\n",
    "            if is_fun\n",
    "            else None\n",
    "        )\n",
    "        for is_fun in cfg.is_retardation_a_func\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnet_up\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata_train_up\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m net_down\u001b[38;5;241m.\u001b[39mrun_training(\u001b[38;5;241m*\u001b[39mdata_train_down)\n",
      "Cell \u001b[0;32mIn[3], line 94\u001b[0m, in \u001b[0;36mConcentrationPredictor.run_training\u001b[0;34m(self, t, u_full_train)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     93\u001b[0m     dt \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 94\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     loss \u001b[38;5;241m=\u001b[39m closure()\n\u001b[1;32m     96\u001b[0m     dt \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m dt\n",
      "File \u001b[0;32m~/.venvs/p3inn/lib/python3.11/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.venvs/p3inn/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/p3inn/lib/python3.11/site-packages/torch/optim/lbfgs.py:445\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter \u001b[38;5;241m!=\u001b[39m max_iter:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 445\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    446\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    447\u001b[0m     opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/.venvs/p3inn/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 87\u001b[0m, in \u001b[0;36mConcentrationPredictor.run_training.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m ret_inv_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretardation_inv_scaled(u_ret)\n\u001b[1;32m     83\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mphys_mult \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m     84\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrelu(ret_inv_pred[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m ret_inv_pred[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m     85\u001b[0m )  \u001b[38;5;66;03m# TODO: mean instead of sum?\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.venvs/p3inn/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/p3inn/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/p3inn/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "net_up.run_training(*data_train_up)\n",
    "net_down.run_training(*data_train_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = {\n",
    "    \"mean\": net_mean,\n",
    "    \"up\": net_up,\n",
    "    \"down\": net_down,\n",
    "}\n",
    "\n",
    "c_up, c_down = compute_boundary_factors(\n",
    "    y_train=y_train.numpy(),\n",
    "    network_preds=eval_networks(networks, x_train, as_numpy=True),\n",
    "    quantile=configs[\"quantile\"],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "pred_train = eval_networks(networks, x_train)\n",
    "pred_valid = eval_networks(networks, x_valid)\n",
    "\n",
    "PICP_train, MPIW_train = caps_calculation(pred_train, c_up, c_down, y_train.numpy())\n",
    "PICP_valid, MPIW_valid = caps_calculation(pred_valid, c_up, c_down, y_valid.numpy())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_train, y_train, \".\")\n",
    "y_U_PI_array_train = (pred_train[\"mean\"] + c_up * pred_train[\"up\"]).numpy().flatten()\n",
    "y_L_PI_array_train = (\n",
    "    (pred_train[\"mean\"] - c_down * pred_train[\"down\"]).numpy().flatten()\n",
    ")\n",
    "y_mean = pred_train[\"mean\"].numpy().flatten()\n",
    "sort_indices = np.argsort(x_train.flatten())\n",
    "ax.plot(x_train.flatten()[sort_indices], y_mean[sort_indices], \"-\")\n",
    "ax.plot(x_train.flatten()[sort_indices], y_U_PI_array_train[sort_indices], \"-\")\n",
    "ax.plot(x_train.flatten()[sort_indices], y_L_PI_array_train[sort_indices], \"-\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3inn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
